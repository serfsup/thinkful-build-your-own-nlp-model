{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge: Build your own NLP model\n",
    "\n",
    "For this challenge, you will need to choose a corpus of data from nltk or another source that includes categories you can predict and create an analysis pipeline that includes the following steps:\n",
    "\n",
    "1. Data cleaning / processing / language parsing\n",
    "2. Create features using two different NLP methods: For example, BoW vs tf-idf\n",
    "3. Use the features to fit supervised learning models for each feature set to predict the category outcomes\n",
    "4. Assess your models using cross-validation and determine whether one model performed better\n",
    "5. Pick one of the models and try to increase accuracy by at least 5 percentage points\n",
    "\n",
    "Write up your report in a Jupyter notebook. Be sure to explicitly justify the choices you make throughout, and submit it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T03:33:42.926790Z",
     "start_time": "2019-07-29T03:33:39.248414Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import data science environment.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from typing import List, Set\n",
    "from nltk.corpus import shakespeare, stopwords\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning, Processing, Language Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T03:33:42.955959Z",
     "start_time": "2019-07-29T03:33:42.933581Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import texts and read the data.\n",
    "with open(r\"./Much Ado About Nothing.txt\", encoding='utf-16') as much_ado:\n",
    "    much_ado_raw = much_ado.read()\n",
    "with open(r\"./Romeo and Juliet.txt\", encoding='utf-16') as romeo:\n",
    "    romeo_raw = romeo.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T03:33:42.966843Z",
     "start_time": "2019-07-29T03:33:42.957883Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function to clean text.\n",
    "def text_cleaner(text:str) -> str :\n",
    "    \"\"\"Function to strip all characters except letters in words.\"\"\"\n",
    "    text = re.sub(r'--', ' ', text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = re.sub(\"[\\<].*?[\\>]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T03:33:42.983436Z",
     "start_time": "2019-07-29T03:33:42.969005Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clean the data.\n",
    "much_ado_clean = text_cleaner(much_ado_raw)\n",
    "romeo_clean = text_cleaner(romeo_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T03:33:43.000761Z",
     "start_time": "2019-07-29T03:33:42.986941Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I learn in this letter that Don Pedro of Arragon comes this night to Messina. He is very near by this: he was not three leagues off when I left him. How many gentlemen have you lost in this action? But few of any sort, and none of name. A victory is twice itself when the achiever brings home full numbers. I find here that Don Pedro hath bestowed much honour on a young Florentine called Claudio. Much deserved on his part and equally remembered by Don Pedro. He hath borne himself beyond the promise of his age, doing in the figure of a lamb the feats of a lion: he hath indeed better bettered expectation than you must expect of me to tell you how. He hath an uncle here in Messina will be very much glad of it. I have already delivered him letters, and there appears much joy in him; even so much that joy could not show itself modest enough without a badge of bitterness. Did he break out into tears? In great measure. A kind overflow of kindness. There are no faces truer than those that are so'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print \"Much Ado\" cleaned text.\n",
    "much_ado_clean[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T03:33:43.010487Z",
     "start_time": "2019-07-29T03:33:43.002842Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Two households, both alike in dignity, In fair Verona, where we lay our scene, From ancient grudge break to new mutiny, Where civil blood makes civil hands unclean. From forth the fatal loins of these two foes A pair of star-cross'd lovers take their life; Whose misadventur'd piteous overthrows Do with their death bury their parents' strife. The fearful passage of their death-mark'd love, And the continuance of their parents' rage, Which, but their children's end, nought could remove, Is now the two hours' traffick of our stage; The which if you with patient ears attend, What here shall miss, our toil shall strive to mend. Gregory, o' my word, we'll not carry coals. No. for then we should be colliers. I mean, an we be in choler, we'll draw. Ay, while you live, draw your neck out o' the collar. I strike quickly, being moved. But thou art not quickly moved to strike. A dog of the house of Montague moves me. To move is to stir, and to be valiant is to stand; therefore, if thou art moved, \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print \"Romeo\" cleaned text.\n",
    "romeo_clean[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T03:33:52.441799Z",
     "start_time": "2019-07-29T03:33:43.015846Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parse the data. This can take some time.\n",
    "nlp = spacy.load('en')\n",
    "much_ado_doc = nlp(much_ado_clean)\n",
    "romeo_doc = nlp(romeo_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T03:33:52.489533Z",
     "start_time": "2019-07-29T03:33:52.446600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(I, learn, in, this, letter, that, Don, Pedro,...</td>\n",
       "      <td>Much Ado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(He, is, very, near, by, this, :, he, was, not...</td>\n",
       "      <td>Much Ado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(How, many, gentlemen, have, you, lost, in, th...</td>\n",
       "      <td>Much Ado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(But, few, of, any, sort, ,, and, none, of, na...</td>\n",
       "      <td>Much Ado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(A, victory, is, twice, itself, when, the, ach...</td>\n",
       "      <td>Much Ado</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0         1\n",
       "0  (I, learn, in, this, letter, that, Don, Pedro,...  Much Ado\n",
       "1  (He, is, very, near, by, this, :, he, was, not...  Much Ado\n",
       "2  (How, many, gentlemen, have, you, lost, in, th...  Much Ado\n",
       "3  (But, few, of, any, sort, ,, and, none, of, na...  Much Ado\n",
       "4  (A, victory, is, twice, itself, when, the, ach...  Much Ado"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "much_ado_sents = [[sent, \"Much Ado\"] for sent in much_ado_doc.sents]\n",
    "romeo_sents = [[sent, \"Romeo\"] for sent in romeo_doc.sents]\n",
    "\n",
    "# Combine the sentences from the two plays into one data frame.\n",
    "sentences = pd.DataFrame(much_ado_sents + romeo_sents)\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T03:33:52.507828Z",
     "start_time": "2019-07-29T03:33:52.493520Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up bag of words function for each text.\n",
    "def bag_of_words(text: spacy.tokens.doc.Doc) -> List:\n",
    "    \"\"\"Counts the total number of instances of each word in a doc.\"\"\"\n",
    "    allwords = [token.lemma_  # extracts base from word, pos (-PRON-)\n",
    "                for token in text\n",
    "                if not token.is_punct  # eliminates punctuation\n",
    "                and not token.is_stop]  # eliminates stop words\n",
    "    var = Counter(allwords).most_common(500)\n",
    "    #print(f\"Counter(allwords).most_common(500) is {var[0]}\")\n",
    "    #print(f\"First element(allwords) is {allwords[0]}\")\n",
    "    #print(f\"Type of first element is {type(allwords[0])}\")\n",
    "    return [item[0] for item in var]\n",
    "\n",
    "\n",
    "def bow_features_common(\n",
    "        sentences: pd.DataFrame, common_words: Set) -> pd.DataFrame:\n",
    "    \"\"\"Takes a Set and a DataFrame.\"\"\"\n",
    "    df = pd.DataFrame(columns=common_words)  # Transforming set to df.\n",
    "    df['text_sentence'] = sentences[0]  # type = spacy.tokens.doc.Doc.\n",
    "    df['text_source'] = sentences[1]  # play label.\n",
    "    df.loc[:, common_words] = 0  # zero all cells except labels.\n",
    "\n",
    "    for i, sentence in enumerate(df['text_sentence']):\n",
    "        common_lemmas = [token.lemma_  # extracts base from word, pos (-PRON-)\n",
    "                         for token in sentence  #\n",
    "                         if (\n",
    "                             not token.is_punct  # eliminates punctuation\n",
    "                             and not token.is_stop  # eliminates stop words\n",
    "                             and token.lemma_ in common_words\n",
    "                         )]\n",
    "        # print(words)\n",
    "        # break\n",
    "\n",
    "        for lemma in common_lemmas:\n",
    "            df.loc[i, lemma] += 1\n",
    "        if i % 100 == 0:\n",
    "            print('Processing row {}'.format(i))\n",
    "    return df  # row of df contains Doc, label, common lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T03:33:52.550180Z",
     "start_time": "2019-07-29T03:33:52.509625Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up bags for each play.\n",
    "much_ado_words = bag_of_words(much_ado_doc)\n",
    "romeo_words = bag_of_words(romeo_doc)\n",
    "\n",
    "# Make bag of common words.\n",
    "common_words = set(much_ado_words + romeo_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T10:48:29.407240Z",
     "start_time": "2019-07-29T03:33:52.552692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0\n",
      "Processing row 100\n",
      "Processing row 200\n",
      "Processing row 300\n",
      "Processing row 400\n",
      "Processing row 500\n",
      "Processing row 600\n",
      "Processing row 700\n",
      "Processing row 800\n",
      "Processing row 900\n",
      "Processing row 1000\n",
      "Processing row 1100\n",
      "Processing row 1200\n",
      "Processing row 1300\n",
      "Processing row 1400\n",
      "Processing row 1500\n",
      "Processing row 1600\n",
      "Processing row 1700\n",
      "Processing row 1800\n",
      "Processing row 1900\n",
      "Processing row 2000\n",
      "Processing row 2100\n",
      "Processing row 2200\n",
      "Processing row 2300\n",
      "Processing row 2400\n",
      "Processing row 2500\n",
      "Processing row 2600\n",
      "Processing row 2700\n",
      "Processing row 2800\n",
      "Processing row 2900\n",
      "Processing row 3000\n",
      "Processing row 3100\n",
      "Processing row 3200\n",
      "Processing row 3300\n",
      "Processing row 3400\n",
      "Processing row 3500\n",
      "Processing row 3600\n",
      "Processing row 3700\n",
      "Processing row 3800\n",
      "Processing row 3900\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>write</th>\n",
       "      <th>liking</th>\n",
       "      <th>face</th>\n",
       "      <th>shalt</th>\n",
       "      <th>change</th>\n",
       "      <th>fair</th>\n",
       "      <th>conrade</th>\n",
       "      <th>blood</th>\n",
       "      <th>ho</th>\n",
       "      <th>goose</th>\n",
       "      <th>...</th>\n",
       "      <th>yet</th>\n",
       "      <th>desperate</th>\n",
       "      <th>for</th>\n",
       "      <th>head</th>\n",
       "      <th>longer</th>\n",
       "      <th>outward</th>\n",
       "      <th>grow</th>\n",
       "      <th>happy</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(I, learn, in, this, letter, that, Don, Pedro,...</td>\n",
       "      <td>Much Ado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(He, is, very, near, by, this, :, he, was, not...</td>\n",
       "      <td>Much Ado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(How, many, gentlemen, have, you, lost, in, th...</td>\n",
       "      <td>Much Ado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(But, few, of, any, sort, ,, and, none, of, na...</td>\n",
       "      <td>Much Ado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(A, victory, is, twice, itself, when, the, ach...</td>\n",
       "      <td>Much Ado</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 697 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  write liking face shalt change fair conrade blood ho goose  ... yet  \\\n",
       "0     0      0    0     0      0    0       0     0  0     0  ...   0   \n",
       "1     0      0    0     0      0    0       0     0  0     0  ...   0   \n",
       "2     0      0    0     0      0    0       0     0  0     0  ...   0   \n",
       "3     0      0    0     0      0    0       0     0  0     0  ...   0   \n",
       "4     0      0    0     0      0    0       0     0  0     0  ...   0   \n",
       "\n",
       "  desperate for head longer outward grow happy  \\\n",
       "0         0   0    0      0       0    0     0   \n",
       "1         0   0    0      0       0    0     0   \n",
       "2         0   0    0      0       0    0     0   \n",
       "3         0   0    0      0       0    0     0   \n",
       "4         0   0    0      0       0    0     0   \n",
       "\n",
       "                                       text_sentence text_source  \n",
       "0  (I, learn, in, this, letter, that, Don, Pedro,...    Much Ado  \n",
       "1  (He, is, very, near, by, this, :, he, was, not...    Much Ado  \n",
       "2  (How, many, gentlemen, have, you, lost, in, th...    Much Ado  \n",
       "3  (But, few, of, any, sort, ,, and, none, of, na...    Much Ado  \n",
       "4  (A, victory, is, twice, itself, when, the, ach...    Much Ado  \n",
       "\n",
       "[5 rows x 697 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up features from BoW.\n",
    "lemma_counts = bow_features_common(sentences, common_words)\n",
    "lemma_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T10:48:29.471017Z",
     "start_time": "2019-07-29T10:48:29.409440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Much Ado: [('i', 712), (\"'s\", 170), ('and', 116), ('man', 111), ('good', 94), ('love', 92), ('thou', 86), ('shall', 82), ('come', 78), ('hath', 76), ('thee', 74), ('lord', 71), ('lady', 66), ('god', 66), ('let', 65), ('hero', 63), ('know', 59), ('claudio', 57), ('benedick', 56), ('prince', 54), ('thy', 52), ('but', 51), ('like', 51), ('o', 49), ('if', 49), (\"'ll\", 48), ('think', 47), ('what', 43), ('you', 43), ('signior', 42), ('hear', 41), ('beatrice', 40), ('tell', 39), ('brother', 38), ('sir', 37), ('the', 37), ('to', 37), ('night', 35), ('no', 35), ('cousin', 34), ('heart', 33), ('that', 33), ('why', 32), ('marry', 32), ('pray', 31), ('daughter', 31), ('speak', 31), ('leonato', 30), ('wit', 29), ('men', 29), ('well', 28), ('yea', 28), ('count', 28), ('in', 27), ('how', 25), ('true', 25), ('doth', 24), ('answer', 24), ('nay', 24), ('my', 23), ('faith', 23), ('is', 23), ('till', 22), ('for', 22), ('morrow', 22), ('he', 21), ('a', 21), ('old', 21), ('hand', 21), ('as', 21), ('she', 21), ('fashion', 20), ('husband', 20), ('look', 20), ('bid', 19), ('by', 19), ('tis', 19), ('day', 19), ('sweet', 19), ('grace', 18), ('said', 18), ('fool', 17), ('fair', 17), ('death', 17), ('margaret', 17), ('great', 16), ('there', 16), ('father', 16), ('die', 16), ('matter', 16), ('away', 16), ('troth', 16), ('we', 16), ('john', 16), ('so', 16), ('don', 15), ('it', 15), ('gentleman', 15), ('leave', 15), ('truly', 15)]\n",
      "Romeo: [('i', 644), ('thou', 278), (\"'s\", 266), ('and', 228), ('thy', 166), ('o', 160), ('love', 149), ('thee', 135), ('romeo', 126), ('shall', 110), ('come', 96), (\"'ll\", 91), ('what', 89), ('night', 87), ('good', 86), ('to', 85), ('that', 79), ('death', 75), ('but', 75), ('man', 71), ('the', 68), ('hath', 63), ('sir', 57), ('tybalt', 57), ('for', 56), ('art', 54), ('a', 53), ('day', 53), ('lady', 52), ('let', 50), ('doth', 47), ('dead', 47), ('fair', 44), ('my', 44), ('tell', 44), ('juliet', 43), ('tis', 41), ('like', 41), ('nurse', 39), ('is', 37), ('sweet', 36), ('this', 36), ('know', 35), ('time', 34), ('heart', 34), ('gone', 34), ('in', 33), ('as', 33), ('god', 33), ('it', 32), ('look', 31), ('where', 30), ('ay', 30), ('wilt', 30), ('or', 30), ('lord', 30), ('word', 29), ('of', 29), ('eyes', 29), ('heaven', 29), ('which', 28), ('true', 28), ('men', 28), ('speak', 28), ('light', 28), ('hast', 28), ('how', 27), ('if', 27), ('so', 27), ('bed', 27), ('stay', 27), ('paris', 27), ('stand', 26), ('comes', 26), ('old', 26), ('by', 26), ('father', 26), ('go', 26), ('dear', 26), ('no', 25), ('why', 25), ('hand', 25), ('life', 24), ('with', 24), ('then', 24), ('find', 24), ('now', 24), ('house', 23), ('away', 23), ('marry', 23), ('madam', 23), ('young', 23), ('die', 23), ('face', 23), ('friar', 23), ('montague', 22), ('you', 22), ('hear', 22), ('here', 22), ('till', 22)]\n"
     ]
    }
   ],
   "source": [
    "# Calculate word frequencies.\n",
    "def word_frequencies(text, include_stop=True):\n",
    "    \"\"\"A data frame that will keep track of word usage.\"\"\"\n",
    "    # Build a list of words.\n",
    "    # Strip out punctuation and stop words.\n",
    "    words = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            words.append(token.text.lower())\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(words)\n",
    "\n",
    "# The most frequent words:\n",
    "much_ado_freq = word_frequencies(\n",
    "    much_ado_doc, include_stop=False).most_common(100)\n",
    "print('Much Ado:', much_ado_freq)\n",
    "romeo_freq = word_frequencies(romeo_doc, include_stop=False).most_common(100)\n",
    "print('Romeo:', romeo_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T10:48:29.488214Z",
     "start_time": "2019-07-29T10:48:29.476683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique to Much Ado: {'grace', 'there', 'truly', 'beatrice', 'brother', 'great', 'daughter', 'answer', 'claudio', 'think', 'matter', 'husband', 'john', 'bid', 'margaret', 'she', 'prince', 'troth', 'signior', 'faith', 'yea', 'he', 'fool', 'morrow', 'leave', 'count', 'leonato', 'don', 'well', 'hero', 'wit', 'fashion', 'said', 'nay', 'cousin', 'gentleman', 'pray', 'benedick', 'we'}\n",
      "Unique to Romeo: {'light', 'word', 'then', 'paris', 'young', 'comes', 'face', 'stay', 'now', 'bed', 'dead', 'madam', 'art', 'stand', 'time', 'dear', 'with', 'romeo', 'eyes', 'ay', 'juliet', 'gone', 'this', 'friar', 'where', 'or', 'wilt', 'tybalt', 'heaven', 'which', 'nurse', 'hast', 'here', 'house', 'of', 'montague', 'life', 'go', 'find'}\n"
     ]
    }
   ],
   "source": [
    "# Pull out just the text from our frequency lists.\n",
    "much_ado_common = [pair[0] for pair in much_ado_freq]\n",
    "romeo_common = [pair[0] for pair in romeo_freq]\n",
    "\n",
    "# Use sets to find the unique values in each top 500.\n",
    "print('Unique to Much Ado:', set(much_ado_common) - set(romeo_common))\n",
    "print('Unique to Romeo:', set(romeo_common) - set(much_ado_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T10:48:29.590714Z",
     "start_time": "2019-07-29T10:48:29.490738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Much Ado: [('-PRON-', 877), ('man', 140), ('and', 116), ('love', 113), (\"'s\", 107), ('good', 105), ('be', 104), ('come', 99), ('thou', 86), ('know', 84), ('shall', 82), ('hath', 76), ('thee', 74), ('lord', 73), ('lady', 69), ('god', 68), ('let', 65), ('hero', 63), ('will', 61), ('think', 61), ('prince', 59), ('tell', 58), ('claudio', 57), ('benedick', 56), ('like', 52), ('thy', 52), ('but', 51), ('hear', 51), ('speak', 51), ('o', 49), ('if', 49), ('what', 43), ('signior', 42), ('beatrice', 40), ('brother', 39), ('marry', 39), ('night', 37), ('sir', 37), ('the', 37), ('to', 37), ('cousin', 36), ('heart', 36), ('no', 35), ('pray', 34), ('that', 33), ('why', 32), ('wit', 31), ('daughter', 31), ('leonato', 30), ('look', 29), ('say', 29), ('go', 28), ('well', 28), ('yea', 28), ('count', 28), ('master', 28), ('in', 27), ('swear', 27), ('die', 27), ('word', 26), ('hand', 26), ('answer', 26), ('how', 25), ('true', 25), ('leave', 24), ('faith', 24), ('doth', 24), ('nay', 24), ('bring', 22), ('fashion', 22), ('till', 22), ('thank', 22), ('for', 22), ('morrow', 22), ('a', 21), ('bear', 21), ('grace', 21), ('old', 21), ('husband', 21), ('as', 21), ('day', 21), ('wear', 20), ('bid', 20), ('eye', 20), ('sweet', 20), ('do', 19), ('great', 19), ('fool', 19), ('friend', 19), ('live', 19), ('fair', 19), ('by', 19), ('tis', 19), ('time', 18), ('lie', 18), ('thing', 18), ('gentleman', 17), ('father', 17), ('troth', 17), ('write', 17), ('death', 17), ('margaret', 17), ('there', 16), ('mean', 16), ('tongue', 16), ('matter', 16), ('away', 16), ('villain', 16), ('john', 16), ('so', 16), ('life', 16), ('don', 15), ('find', 15), ('appear', 15), ('kill', 15), ('hang', 15), ('truly', 15), ('way', 15), ('wrong', 15), ('send', 15), ('stand', 15), ('see', 15), ('which', 15), ('soul', 15), ('face', 14), ('meet', 14), ('right', 14), ('talk', 14), ('blood', 14), ('sad', 14), ('wise', 14), ('break', 13), ('niece', 13), ('help', 13), ('hold', 13), ('charge', 13), ('child', 13), ('woman', 13), ('this', 13), ('dare', 13), ('honest', 13), ('dost', 13), ('fellow', 13), ('marriage', 13), ('not', 13), ('ill', 13), ('slander', 13), ('young', 12), ('uncle', 12), ('eat', 12), ('excellent', 12), ('world', 12), ('praise', 12), ('turn', 12), ('prove', 12), ('sure', 12), ('give', 12), ('warrant', 12), ('watch', 12), ('neighbour', 12), ('friar', 12), ('who', 11), ('ere', 11), ('bad', 11), ('note', 11), ('fall', 11), ('here', 11), ('hour', 11), ('strange', 11), ('art', 11), ('affection', 11), ('truth', 11), ('counsel', 11), ('worship', 11), ('honour', 10), ('joy', 10), ('virtue', 10), ('merry', 10), ('thousand', 10), ('end', 10), ('little', 10), ('trust', 10), ('wilt', 10), ('horn', 10), ('long', 10), ('cry', 10), ('sing', 10), ('woo', 10), ('maid', 10), ('ear', 10), ('boy', 10), ('poor', 10), ('now', 10), ('ha', 10), ('false', 10), ('window', 10), ('shame', 10), ('dead', 10), ('messina', 9), ('call', 9), ('cupid', 9), ('noble', 9), ('presently', 9), ('head', 9), ('dear', 9), ('then', 9), ('mind', 9), ('sigh', 9), ('when', 9), ('grief', 9), ('music', 9), ('walk', 9), ('use', 9), ('yet', 9), ('hither', 9), ('one', 9), ('heavy', 9), ('pain', 9), ('foul', 9), ('age', 8), ('measure', 8), ('challenge', 8), ('get', 8), ('possible', 8), ('take', 8), ('wonder', 8), ('stay', 8), ('question', 8), ('would', 8), ('can', 8), ('may', 8), ('follow', 8), ('thought', 8), ('have', 8), ('desire', 8), ('all', 8), ('serve', 8), ('fit', 8), ('dance', 8), ('care', 8), ('laugh', 8), ('deny', 8), ('morning', 8), ('endure', 8), ('beard', 8), ('light', 8), ('sit', 8), ('believe', 8), ('villany', 8), ('office', 8), ('farewell', 8), ('steal', 8), ('sick', 8), ('chamber', 8), ('scorn', 8), ('spirit', 8), ('fear', 8), ('peace', 8), ('fare', 8), ('constable', 8), ('innocent', 8), ('ass', 8), ('near', 7), ('remember', 7), ('better', 7), ('kind', 7), ('new', 7), ('company', 7), ('humour', 7), ('flout', 7), ('wife', 7), ('seek', 7), ('with', 7), ('tale', 7), ('bull', 7), ('examine', 7), ('son', 7), ('reason', 7), ('hide', 7), ('jest', 7), ('melancholy', 7), ('youth', 7), ('sin', 7), ('nor', 7), ('save', 7), ('proof', 7), ('bind', 7), ('writ', 7), ('patience', 7), (\"lov'd\", 7), ('offence', 7), ('hast', 7), ('learn', 6), ('pedro', 6), ('lose', 6), ('deserve', 6), ('tear', 6), ('weep', 6), ('war', 6), ('ask', 6), ('set', 6), ('honourable', 6), ('horse', 6), ('change', 6), ('run', 6), ('house', 6), ('mark', 6), ('exceed', 6), ('hope', 6), ('passion', 6), ('worthy', 6), ('despite', 6), ('fine', 6), ('body', 6), ('thine', 6), ('than', 6), ('place', 6), ('discover', 6), ('curst', 6), ('show', 6), ('yes', 6), ('wedding', 6), ('beat', 6), ('against', 6), ('wish', 6), ('hurt', 6), ('quarrel', 6), ('offend', 6), ('draw', 6), ('witness', 6), ('sorry', 6), ('surely', 6), ('ursula', 6), ('madam', 6), ('shape', 6), ('knave', 6), ('thief', 6), ('unto', 6), ('accuse', 6), ('letter', 5), ('bestow', 5), ('modest', 5), ('doubt', 5), ('soldier', 5), ('stuff', 5), ('book', 5), ('catch', 5), ('encounter', 5), ('depart', 5), ('disdain', 5), ('convert', 5), ('certain', 5), ('rare', 5), ('low', 5), ('play', 5), ('beauty', 5), ('utter', 5), ('fire', 5), ('wind', 5), ('pardon', 5), ('because', 5), ('sign', 5), ('argument', 5), ('savage', 5), ('married', 5), ('supper', 5), ('mock', 5), ('liking', 5), ('practice', 5), ('news', 5), ('outward', 5), ('orchard', 5), ('moral', 5), ('sleep', 5), ('impossible', 5), ('mouth', 5), ('proper', 5), ('cross', 5), ('half', 5), ('gentlewoman', 5), ('ape', 5), ('match', 5), ('suit', 5), ('church', 5), ('favour', 5), ('sake', 5), ('devise', 5), ('thus', 5), ('breath', 5), ('purpose', 5), ('hair', 5), ('indeed', 5), ('issue', 5), ('virtuous', 5), ('rich', 5), ('worth', 5), ('woe', 5), ('into', 5), ('of', 5), ('fancy', 5), ('conclude', 5), ('suffer', 5), ('deform', 5), ('tomb', 5), ('flee', 5), ('rime', 5), ('sort', 4), ('read', 4), ('bird', 4), ('valiant', 4), ('stomach', 4), ('alas', 4), ('hat', 4), ('mad', 4), ('trouble', 4), ('never', 4), ('comfort', 4), ('happy', 4), ('hard', 4), ('longer', 4), ('lead', 4), ('methink', 4), ('high', 4), ('sport', 4), ('song', 4), ('possess', 4), ('bachelor', 4), ('need', 4), ('allegiance', 4), ('short', 4), ('twa', 4), ('forbid', 4), ('shortly', 4), ('amen', 4), ('fetch', 4), ('wouldst', 4), ('meantime', 4), ('commit', 4), ('teach', 4), ('shalt', 4), ('strong', 4), ('cover', 4), ('dream', 4), ('year', 4), ('mischief', 4), (\"ta'en\", 4), ('plain', 4), ('borachio', 4), ('conference', 4), ('foot', 4), ('money', 4), ('win', 4), ('apparel', 4), ('piece', 4), ('enter', 4), ('visor', 4), ('counterfeit', 4), ('gift', 4), ('strike', 4), ('therefore', 4), ('garland', 4), ('begin', 4), ('double', 4), ('wherefore', 4), ('fortune', 4), ('beseech', 4), ('wake', 4), ('rite', 4), ('brief', 4), ('strain', 4), ('valour', 4), ('impediment', 4), ('cunning', 4), ('accusation', 4), ('doublet', 4), ('sound', 4), ('nonny', 4), ('an', 4), ('reverence', 4), ('paper', 4), ('pretty', 4), ('wisdom', 4), ('dinner', 4), ('ready', 4), ('pity', 4), ('proud', 4), ('cut', 4), ('bed', 4), ('nature', 4), ('out', 4), ('some', 4), ('maiden', 4), ('holy', 4), ('tooth', 4), ('ache', 4), ('lay', 4), ('conrade', 4), ('tush', 4), ('forth', 4), ('gown', 4)]\n",
      "Romeo: [('-PRON-', 855), ('thou', 278), ('and', 228), (\"'s\", 193), ('thy', 166), ('o', 160), ('love', 156), ('be', 144), ('come', 139), ('thee', 135), ('romeo', 126), ('shall', 110), ('will', 103), ('good', 100), ('man', 99), ('what', 89), ('night', 88), ('to', 85), ('that', 79), ('death', 75), ('but', 75), ('the', 69), ('go', 68), ('hath', 63), ('day', 62), ('sir', 57), ('tybalt', 57), ('lady', 57), ('for', 56), ('art', 54), ('a', 53), ('let', 52), ('fair', 47), ('eye', 47), ('tell', 47), ('doth', 47), ('dead', 47), ('know', 45), ('lie', 45), ('juliet', 43), ('time', 42), ('like', 42), ('tis', 41), ('speak', 41), ('look', 40), ('sweet', 40), ('nurse', 40), ('hand', 38), ('marry', 38), ('heart', 37), ('word', 36), ('this', 36), ('in', 33), ('live', 33), ('as', 33), ('find', 33), ('god', 33), ('heaven', 31), ('where', 30), ('ay', 30), ('wilt', 30), ('or', 30), ('stay', 30), ('lord', 30), ('stand', 29), ('light', 29), ('of', 29), ('hast', 29), ('which', 28), ('true', 28), ('say', 28), ('old', 28), ('dear', 28), ('house', 27), ('how', 27), ('if', 27), ('so', 27), ('bed', 27), ('die', 27), ('think', 27), ('paris', 27), ('life', 26), ('hour', 26), ('hear', 26), ('by', 26), ('tear', 26), ('young', 26), ('father', 26), ('no', 25), ('montague', 25), ('why', 25), ('see', 25), ('give', 24), ('with', 24), ('then', 24), ('now', 24), ('slay', 24), ('away', 23), ('madam', 23), ('thing', 23), ('face', 23), ('friar', 23), ('do', 22), ('here', 22), ('till', 22), ('gentleman', 22), ('woe', 22), ('bear', 21), ('have', 21), ('morrow', 21), ('farewell', 21), ('bid', 21), ('holy', 21), ('mean', 20), ('head', 20), ('nay', 20), ('hold', 20), ('prince', 20), ('ere', 20), ('earth', 20), ('help', 20), ('make', 19), ('child', 19), ('run', 19), ('well', 19), ('peace', 19), ('long', 19), ('who', 19), ('cousin', 19), ('world', 19), ('letter', 19), ('wife', 19), ('mercutio', 19), ('banish', 19), ('blood', 18), ('son', 18), ('friend', 18), ('leave', 18), ('ah', 18), ('take', 18), ('daughter', 18), ('faith', 18), ('lip', 18), ('ear', 17), ('maid', 17), ('poor', 17), ('villain', 17), ('grave', 17), ('rest', 17), ('an', 17), ('wit', 17), ('mother', 17), ('send', 17), ('pray', 17), ('county', 17), ('fall', 17), ('dream', 17), ('flower', 17), ('draw', 16), ('turn', 16), ('fear', 16), ('kinsman', 16), ('sun', 16), ('early', 16), ('beauty', 16), ('dost', 16), ('grief', 16), ('ill', 16), ('call', 16), ('joy', 16), ('new', 15), ('quarrel', 15), ('capulet', 15), ('soon', 15), ('sleep', 15), ('year', 15), ('breath', 15), ('bring', 15), ('news', 15), ('hate', 14), ('sound', 14), ('not', 14), ('gentle', 14), ('weep', 14), ('thine', 14), ('when', 14), ('poison', 14), ('need', 14), ('soul', 14), ('sin', 14), ('haste', 14), ('thursday', 14), ('lay', 13), ('master', 13), ('talk', 13), ('ho', 13), ('swear', 13), ('husband', 13), ('use', 13), ('verona', 12), ('forth', 12), ('lover', 12), ('law', 12), ('happy', 12), ('saint', 12), ('forget', 12), ('kiss', 12), ('mistress', 12), (\"o'er\", 12), ('ye', 12), ('mantua', 12), ('little', 12), ('some', 12), ('boy', 12), ('cheek', 12), ('hither', 12), ('shame', 12), ('meet', 12), ('kill', 12), ('tongue', 12), ('cell', 12), ('end', 11), ('wall', 11), ('show', 11), ('serve', 11), ('shalt', 11), ('foot', 11), ('place', 11), ('set', 11), ('morning', 11), ('black', 11), ('way', 11), ('hide', 11), ('comfort', 11), ('great', 11), ('sit', 11), ('warrant', 11), ('marriage', 11), ('late', 11), ('watch', 11), ('therefore', 11), ('pale', 11), ('alack', 11), ('pardon', 11), ('peter', 11), ('body', 11), ('there', 11), ('feel', 10), ('dare', 10), ('sword', 10), ('beat', 10), ('ground', 10), ('heavy', 10), ('sorrow', 10), ('rich', 10), ('one', 10), ('desperate', 10), ('mad', 10), ('fortune', 10), ('rosaline', 10), (\"ne'er\", 10), ('than', 10), ('straight', 10), ('play', 10), ('answer', 10), ('murder', 10), ('silver', 10), ('church', 10), ('ancient', 9), ('begin', 9), ('fool', 9), ('seek', 9), ('fire', 9), ('on', 9), ('case', 9), ('mind', 9), ('sick', 9), ('nor', 9), ('unto', 9), ('write', 9), ('book', 9), ('ask', 9), ('high', 9), ('follow', 9), ('torch', 9), ('hang', 9), ('wear', 9), ('thank', 9), ('vault', 9), ('foe', 8), ('star', 8), ('bury', 8), ('pretty', 8), ('bite', 8), ('enemy', 8), ('measure', 8), ('cloud', 8), ('counsel', 8), ('because', 8), ('noble', 8), ('alas', 8), ('yet', 8), ('wake', 8), ('sea', 8), ('welcome', 8), ('burn', 8), ('fellow', 8), ('merry', 8), ('sight', 8), ('cry', 8), ('finger', 8), ('bone', 8), ('bosom', 8), ('vile', 8), ('arm', 8), ('bad', 8), ('music', 8), ('dry', 8), ('tomb', 8), ('goose', 8), ('hie', 8), ('all', 8), ('woeful', 8), ('break', 7), ('fearful', 7), ('strike', 7), ('move', 7), ('stir', 7), ('woman', 7), ('fight', 7), ('flesh', 7), ('remember', 7), ('bloody', 7), ('wind', 7), ('chamber', 7), ('prove', 7), ('air', 7), ('grow', 7), ('short', 7), ('cold', 7), ('at', 7), ('near', 7), ('mark', 7), ('hit', 7), ('brow', 7), ('note', 7), ('canst', 7), ('feast', 7), ('behold', 7), ('dark', 7), ('writ', 7), ('brother', 7), ('girl', 7), ('thousand', 7), ('lend', 7), ('fly', 7), ('beseech', 7), ('too', 7), ('anon', 7), ('more', 7), ('hot', 7), ('past', 7), ('fetch', 7), ('honest', 7), ('would', 7), ('thyself', 7), ('sudden', 7), ('laurence', 7), ('corse', 7), ('proud', 7), ('from', 6), ('dog', 6), ('weak', 6), ('hadst', 6), ('thumb', 6), ('better', 6), ('benvolio', 6), ('noise', 6), ('street', 6), ('close', 6), ('hurt', 6), ('deep', 6), ('sigh', 6), ('home', 6), ('envious', 6), ('form', 6), ('lead', 6), ('breast', 6), ('soft', 6), ('these', 6), ('read', 6), ('bind', 6), ('change', 6), ('fourteen', 6), ('consent', 6), ('voice', 6), ('sirrah', 6), ('shut', 6), ('den', 6), ('matter', 6), ('awhile', 6), ('age', 6), ('dove', 6), ('jest', 6), ('wast', 6), ('wish', 6), ('strength', 6), ('excuse', 6), ('tender', 6), ('care', 6), ('joint', 6), ('ha', 6), ('tale', 6), ('musician', 6), ('yonder', 6), ('youth', 6), ('strange', 6), ('nature', 6), ('tree', 6), ('kind', 6), ('stop', 6), ('didst', 6), ('company', 6), ('ring', 6), ('wild', 6), ('thought', 6), ('fie', 6), ('simple', 6), ('get', 6), ('work', 6), ('reason', 6), ('doom', 6), (\"kill'd\", 6), ('point', 6), ('exile', 6), ('sell', 6), ('banishment', 6), ('lark', 6), ('monument', 6), ('churchyard', 6), ('two', 5), ('piteous', 5), ('attend', 5), ('quickly', 5), ('cut', 5), ('pass', 5), ('blow', 5), ('hell', 5), ('pain', 5), ('depart', 5), ('pleasure', 5), ('town', 5), ('withal', 5), ('scorn', 5), ('right', 5), ('window', 5), ('walk', 5), ('steal', 5), ('affection', 5), ('dew', 5), ('learn', 5), ('far', 5), ('could', 5), ('deny', 5), ('shrift', 5), ('sad', 5), ('out', 5), ('rough', 5), ('bright', 5), ('lose', 5), ('groan', 5), ('cupid', 5), ('gold', 5), ('wise', 5), ('wisely', 5), ('blind', 5), ('honourable', 5), ('bride', 5), ('hope', 5), ('woo', 5), ('guest', 5), ('delight', 5), ('may', 5), ('keep', 5), (\"know'st\", 5), ('brain', 5), ('quoth', 5), ('knock', 5), ('i.', 5), ('grace', 5), ('honour', 5), ('count', 5), ('brief', 5), ('wing', 5), ('rude', 5), ('asleep', 5), ('shape', 5), ('half', 5), ('state', 5), ('prayer', 5)]\n",
      "Unique to Much Ado: {'false', 'passion', 'certain', 'stuff', 'liking', 'sure', 'knave', 'wherefore', 'conrade', 'offence', 'valour', 'patience', 'slander', 'forbid', 'amen', 'horn', 'save', 'accuse', 'piece', 'ready', 'purpose', 'spirit', 'truth', 'fancy', 'shortly', 'signior', 'yea', 'neighbour', 'fare', 'virtue', 'wisdom', 'eat', 'meantime', 'examine', 'practice', 'visor', 'ass', 'valiant', 'conclude', 'double', 'despite', 'fashion', 'proof', 'offend', 'foul', 'worth', 'modest', 'innocent', 'desire', 'commit', 'sake', 'appear', 'sorry', 'deform', 'pity', 'argument', 'laugh', 'gift', 'truly', 'messina', 'match', 'doublet', 'rime', 'discover', 'can', 'trust', 'apparel', 'war', 'question', 'john', 'twa', 'thief', 'borachio', 'nonny', 'doubt', 'ache', 'margaret', 'virtuous', 'wouldst', 'charge', 'troth', 'endure', 'against', 'tooth', 'maiden', 'disdain', 'melancholy', 'hat', 'fine', 'conference', 'leonato', 'worship', 'low', 'impossible', 'strong', 'horse', 'paper', 'rite', 'never', 'possible', 'beard', 'reverence', 'pedro', 'surely', 'excellent', 'deserve', 'suffer', 'uncle', 'utter', 'counterfeit', 'married', 'beatrice', 'cross', 'sort', 'devise', \"lov'd\", 'strain', 'ape', 'claudio', 'bull', 'rare', 'sing', 'allegiance', 'garland', 'savage', \"ta'en\", 'indeed', 'worthy', 'fit', 'mischief', 'orchard', 'don', 'sport', 'cover', 'mouth', 'plain', 'niece', 'villany', 'catch', 'hair', 'encounter', 'trouble', 'impediment', 'flee', 'constable', 'office', 'bird', 'witness', 'dinner', 'possess', 'wedding', 'favour', 'dance', 'mock', 'wrong', 'gown', 'bachelor', 'soldier', 'issue', 'into', 'wonder', 'curst', 'tush', 'accusation', 'song', 'gentlewoman', 'suit', 'methink', 'convert', 'sign', 'exceed', 'challenge', 'praise', 'enter', 'moral', 'win', 'bestow', 'supper', 'stomach', 'believe', 'money', 'yes', 'hero', 'cunning', 'humour', 'presently', 'flout', 'benedick', 'hard', 'teach', 'longer', 'outward', 'thus', 'proper', 'ursula'}\n",
      "Unique to Romeo: {'home', 'paris', 'fly', 'fourteen', 'forget', 'dew', 'flesh', 'envious', 'ho', 'goose', 'laurence', \"kill'd\", 'soft', 'earth', 'blind', 'wall', 'point', 'slay', 'dry', 'withal', 'knock', 'vile', 'on', 'hell', 'banishment', 'fie', 'kinsman', 'street', 'romeo', 'dog', 'consent', 'benvolio', 'fearful', 'form', 'gentle', 'pass', 'quoth', 'monument', 'piteous', 'wast', 'stir', 'excuse', 'enemy', 'mantua', 'gold', 'straight', 'noise', 'didst', 'prayer', 'saint', 'wild', 'close', 'shut', 'sight', 'far', 'dark', 'at', 'woeful', 'corse', 'exile', 'haste', 'ground', 'bride', 'rough', 'lover', 'burn', 'welcome', 'sun', 'strength', 'finger', 'thyself', 'feast', 'town', 'banish', 'black', 'too', 'quickly', 'lend', 'vault', 'from', 'work', 'sirrah', 'county', 'anon', 'tree', 'attend', 'feel', 'past', 'ye', 'cell', 'could', 'alack', 'these', 'rude', 'sudden', 'cold', 'poison', 'peter', 'bone', 'rest', 'ring', 'flower', 'groan', 'voice', 'bite', \"know'st\", 'thursday', 'mother', 'canst', 'hate', 'two', 'star', 'guest', 'pale', \"ne'er\", 'brain', 'bury', 'bosom', 'sell', 'simple', 'pleasure', 'cloud', 'ay', 'air', 'ah', 'case', 'keep', 'joint', 'where', 'or', 'dove', 'hot', 'state', 'lark', 'ancient', 'early', 'shrift', 'nurse', 'hie', 'brow', 'stop', 'silver', 'verona', 'yonder', 'behold', 'montague', 'wing', 'rosaline', 'blow', 'murder', 'sea', 'arm', 'grave', 'den', 'more', 'mistress', 'sorrow', 'bloody', 'cheek', 'mercutio', 'foe', 'thumb', 'torch', 'hit', 'law', 'awhile', 'churchyard', \"o'er\", 'i.', 'capulet', 'soon', 'tender', 'asleep', 'musician', 'late', 'juliet', 'bright', 'girl', 'hadst', 'fight', 'make', 'wisely', 'deep', 'heaven', 'kiss', 'lip', 'tybalt', 'weak', 'sword', 'breast', 'desperate', 'move', 'grow', 'doom', 'delight'}\n"
     ]
    }
   ],
   "source": [
    "# Utility function to calculate how frequently lemmas appear in the text.\n",
    "def lemma_frequencies(text, include_stop=True):\n",
    "    \"\"\"Function to identify lemma frequencies\"\"\"\n",
    "    # Build a list of lemmas.\n",
    "    # Strip out punctuation and stop words.\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        if not token.is_punct and (not token.is_stop or include_stop):\n",
    "            lemmas.append(token.lemma_)\n",
    "            \n",
    "    # Build and return a Counter object containing word counts.\n",
    "    return Counter(lemmas)\n",
    "\n",
    "# Instantiate our list of most common lemmas.\n",
    "much_ado_lemma_freq = lemma_frequencies(\n",
    "    much_ado_doc, include_stop=False).most_common(500)\n",
    "romeo_lemma_freq = lemma_frequencies(\n",
    "    romeo_doc, include_stop=False).most_common(500)\n",
    "print('\\nMuch Ado:', much_ado_lemma_freq)\n",
    "print('Romeo:', romeo_lemma_freq)\n",
    "\n",
    "# Again, identify the lemmas common to one text but not the other.\n",
    "much_ado_lemma_common = [pair[0] for pair in much_ado_lemma_freq]\n",
    "romeo_lemma_common = [pair[0] for pair in romeo_lemma_freq]\n",
    "\n",
    "print('Unique to Much Ado:', set(much_ado_lemma_common) -\n",
    "      set(romeo_lemma_common))\n",
    "print('Unique to Romeo:', set(romeo_lemma_common) -\n",
    "      set(much_ado_lemma_common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T10:48:29.604110Z",
     "start_time": "2019-07-29T10:48:29.593141Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Much Ado About Nothing has 1685 sentences.\n",
      "Romeo and Juliet has 2230 sentences.\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many sentences are in each play.\n",
    "sents_much_ado = list(much_ado_doc.sents)\n",
    "sents_romeo = list(romeo_doc.sents)\n",
    "\n",
    "print(\"Much Ado About Nothing has {} sentences.\".format(len(sents_much_ado)))\n",
    "print(\"Romeo and Juliet has {} sentences.\".format(len(sents_romeo)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T10:48:29.620025Z",
     "start_time": "2019-07-29T10:48:29.608354Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Copy lemma_counts data frame as a form of version control.\\nlemma_counts2 = word_counts\\n\\n# Add a column for the lemma counts in each sentence to the data frame.\\nlemma_counts2['sent_length'] = lemma_counts2.text_sentence.map(\\n    lambda x: len(x))\\n\\n# Let's create a count for parts of speech.\\n# Adverbs in each sentence.\\nsentences = lemma_counts2.text_sentence\\nadv_count = []\\nfor sent in sentences:\\n    advs = 0\\n    for token in sent:\\n        if token.pos_ == 'ADV':\\n            advs += 1\\n    adv_count.append(advs)\\n\\n# Add adverbs column to data frame.\\nlemma_counts2['adv_count'] = adv_count\\n\\n# Verbs in each sentence.\\nverb_count = []\\nfor sent in sentences:\\n    verb = 0\\n    for token in sent:\\n        if token.pos_ == 'VERB':\\n            verb += 1\\n    verb_count.append(verb)\\n\\n# Add verbs column to data frame.\\nlemma_counts2['verb_count'] = verb_count\\n\\n# Nouns in each sentence:\\nnoun_count = []\\nfor sent in sentences:\\n    noun = 0\\n    for token in sent:\\n        if token.pos_ == 'NOUN':\\n            noun += 1\\n    noun_count.append(noun)\\n\\n# Add nouns column to data frame.\\nlemma_counts2['noun_count'] = noun_count\\n\\n# Punctuation marks in each sentence.\\npunct_count = []\\nfor sent in sentences:\\n    punct = 0\\n    for token in sent:\\n        if token.pos_ == 'PUNCT':\\n            punct += 1\\n    punct_count.append(punct)\\n\\n# Add punctuation column to data frame.\\nlemma_counts2['punct_count'] = punct_count\\n\\nlemma_counts2.head()\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Copy lemma_counts data frame as a form of version control.\n",
    "lemma_counts2 = word_counts\n",
    "\n",
    "# Add a column for the lemma counts in each sentence to the data frame.\n",
    "lemma_counts2['sent_length'] = lemma_counts2.text_sentence.map(\n",
    "    lambda x: len(x))\n",
    "\n",
    "# Let's create a count for parts of speech.\n",
    "# Adverbs in each sentence.\n",
    "sentences = lemma_counts2.text_sentence\n",
    "adv_count = []\n",
    "for sent in sentences:\n",
    "    advs = 0\n",
    "    for token in sent:\n",
    "        if token.pos_ == 'ADV':\n",
    "            advs += 1\n",
    "    adv_count.append(advs)\n",
    "\n",
    "# Add adverbs column to data frame.\n",
    "lemma_counts2['adv_count'] = adv_count\n",
    "\n",
    "# Verbs in each sentence.\n",
    "verb_count = []\n",
    "for sent in sentences:\n",
    "    verb = 0\n",
    "    for token in sent:\n",
    "        if token.pos_ == 'VERB':\n",
    "            verb += 1\n",
    "    verb_count.append(verb)\n",
    "\n",
    "# Add verbs column to data frame.\n",
    "lemma_counts2['verb_count'] = verb_count\n",
    "\n",
    "# Nouns in each sentence:\n",
    "noun_count = []\n",
    "for sent in sentences:\n",
    "    noun = 0\n",
    "    for token in sent:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            noun += 1\n",
    "    noun_count.append(noun)\n",
    "\n",
    "# Add nouns column to data frame.\n",
    "lemma_counts2['noun_count'] = noun_count\n",
    "\n",
    "# Punctuation marks in each sentence.\n",
    "punct_count = []\n",
    "for sent in sentences:\n",
    "    punct = 0\n",
    "    for token in sent:\n",
    "        if token.pos_ == 'PUNCT':\n",
    "            punct += 1\n",
    "    punct_count.append(punct)\n",
    "\n",
    "# Add punctuation column to data frame.\n",
    "lemma_counts2['punct_count'] = punct_count\n",
    "\n",
    "lemma_counts2.head()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T10:48:29.637615Z",
     "start_time": "2019-07-29T10:48:29.626138Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.feature_extraction.text.TfidfVectorizer"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new variables that aren't spacy tokens.\n",
    "much_ado_tfidf = much_ado_clean\n",
    "romeo_tfidf = romeo_clean\n",
    "\n",
    "much_ado_tfidf = TfidfVectorizer(much_ado_tfidf)\n",
    "type(much_ado_tfidf)\n",
    "#df_much_ado = pd.DataFrame(much_ado_tfidf)\n",
    "# Group into sentences.\n",
    "#much_ado_tfidf_sents = [[sent, \"Much Ado\"] for sent in much_ado_tfidf.split()]\n",
    "#romeo_tfidf_sents = [[sent, \"Romeo\"] for sent in romeo_tfidf.split()]\n",
    "\n",
    "# Combine sentences from two plays into one data frame.\n",
    "#sentences_tfidf = pd.DataFrame(much_ado_tfidf_sents + romeo_tfidf_sents)\n",
    "#sentences_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating training/testing splits for both data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T10:48:29.921611Z",
     "start_time": "2019-07-29T10:48:29.644998Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting to train/test for Bag of Words.\n",
    "Y_bow = lemma_counts['text_source']\n",
    "X_bow = lemma_counts.drop(['text_sentence', 'text_source'], 1).values\n",
    "\n",
    "# Create train, test sets for model.\n",
    "X_bow_train, X_bow_test, y_bow_train, y_bow_test = train_test_split(\n",
    "    X_bow,  # ndarray\n",
    "    Y_bow,  # Series\n",
    "    test_size=0.25,\n",
    "    random_state=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T10:48:30.164628Z",
     "start_time": "2019-07-29T10:48:29.923999Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a7933b7367bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Splitting to train/test for Tfidf.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mY_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_tfidf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_source'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_sentence'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text_source'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Create train, test sets for model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentences_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "# Splitting to train/test for Tfidf.\n",
    "Y_tfidf = sentences_tfidf['text_source']\n",
    "X_tfidf = sentences_tfidf.drop(['text_sentence', 'text_source'], 1).values\n",
    "\n",
    "# Create train, test sets for model.\n",
    "X_tfidf_train, X_tfidf_test, y_tfidf_train, y_tfidf_test = train_test_split(\n",
    "    X_tfidf,  # ndarray\n",
    "    Y_tfidf,  # Series\n",
    "    test_size=0.25,\n",
    "    random_state=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T10:48:30.172939Z",
     "start_time": "2019-07-29T03:33:39.246Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T10:48:30.175264Z",
     "start_time": "2019-07-29T03:33:39.248Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a function to fit and show our predictive models.\n",
    "def fit_and_predict(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f'The {model} model scored {model.score(X_train, y_train)} on train.')\n",
    "    print(f'The {model} model scored {model.score(X_test, y_test)} on test')\n",
    "    y_pred = model.predict(y_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T10:48:30.177920Z",
     "start_time": "2019-07-29T03:33:39.251Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate Decision Tree Classifier.\n",
    "dtc = DecisionTreeClassifier()\n",
    "fit_and_predict(dtc, X_bow_train, y_bow_train\n",
    "# Fit model.\n",
    "dtc.fit(X_train, y_train)\n",
    "\n",
    "# Print training and test scores.\n",
    "print('Training set score: ', dtc.score(X_train, y_train))\n",
    "print('\\nTest set score:', dtc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T10:48:30.179804Z",
     "start_time": "2019-07-29T03:33:39.254Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate Random Forest Classifier.\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "\n",
    "# Fit model.\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Print training and test scores.\n",
    "print('Training set score: ', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T10:48:30.183179Z",
     "start_time": "2019-07-29T03:33:39.256Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate Logistic Regression Classifier.\n",
    "lr = LogisticRegression()\n",
    "# Fit model.\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Print shape, training and test scores.\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T10:48:30.185147Z",
     "start_time": "2019-07-29T03:33:39.258Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate Gradient Boosting Classifier.\n",
    "clf = GradientBoostingClassifier()\n",
    "# Fit model.\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Print shape, training and test scores.\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T10:48:30.187952Z",
     "start_time": "2019-07-29T03:33:39.260Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate Support Vector model.\n",
    "svc = LinearSVC()\n",
    "# Fit model.\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Print training and test set scores.\n",
    "print('Training set score:', svc.score(X_train, y_train))\n",
    "print('\\nTest set score:', svc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T10:48:30.190123Z",
     "start_time": "2019-07-29T03:33:39.266Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's go back and re-try SVM with the new features.\n",
    "Y = lemma_counts2['text_source']\n",
    "X = np.array(lemma_counts2.drop(['text_sentence', 'text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=15\n",
    "                                                   )\n",
    "svm2 = LinearSVC()\n",
    "train = svm2.fit(X_train, y_train)\n",
    "print('Training set score: ', svm2.score(X_train, y_train))\n",
    "print('\\nTest set score: ', svm2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T10:48:30.192754Z",
     "start_time": "2019-07-29T03:33:39.268Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's see this in crosstab.\n",
    "svm2_predicted = svm2.predict(X_test)\n",
    "pd.crosstab(y_test, svm2_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T10:48:30.196438Z",
     "start_time": "2019-07-29T03:33:39.270Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's try the Random Forest again with the new features.\n",
    "rfc2 = ensemble.RandomForestClassifier()\n",
    "Y = lemma_counts2['text_source']\n",
    "X = np.array(lemma_counts2.drop(['text_sentence', 'text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    Y,\n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=15\n",
    "                                                   )\n",
    "\n",
    "train = rfc2.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score: ', rfc2.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T10:48:30.199500Z",
     "start_time": "2019-07-29T03:33:39.273Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's set up our model again.\n",
    "Y = lemma_counts2['text_source']\n",
    "X = np.array(lemma_counts2.drop(['text_sentence', 'text_source'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    Y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=15\n",
    "                                                   )\n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=5000)\n",
    "train = lr.fit(X_train, y_train)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying this many different ways with many Q & A Session visits with multiple mentors, I am unable to improve upon the results of these models. Some of the models do improve as we go on, but nothing close to the 5% improvement that is requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "479px",
    "left": "1126px",
    "right": "20px",
    "top": "12px",
    "width": "270px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
